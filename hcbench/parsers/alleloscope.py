import numpy as np
import pandas as pd
import os

from .cna_parser_base import CNAParser
from .utils import split_region


class AlleloscopeParser(CNAParser):
    """
    Parser for Alleloscope output files.

    This class reads RDS files generated by Alleloscope and converts them
    into a unified haplotype-level CNA table suitable for downstream analysis.
    """

    def __init__(
        self,
        genotypes_rds_path,
        seg_table_rds_path,
        output_path,
        bin_size=None,
        add_chr_prefix=True,
        start_plus_one=True
    ):
        super().__init__(input_path=None, output_path=output_path)
        self.genotypes_rds_path = genotypes_rds_path
        self.seg_table_rds_path = seg_table_rds_path
        self.bin_size = bin_size
        self.add_chr_prefix = add_chr_prefix
        self.start_plus_one = start_plus_one

    def before_pivot(self):
        """
        Load and preprocess Alleloscope genotypes and segmentation tables.

        Returns:
            pd.DataFrame: A dataframe of regions (rows) Ã— cells (columns)
            with combined haplotype CN states (e.g., "1|1").
        """
        try:
            import pyreadr
            result = pyreadr.read_r(self.genotypes_rds_path)
            gt = list(result.values())[0]

            result_seg = pyreadr.read_r(self.seg_table_rds_path)
            seg_table = list(result_seg.values())[0]

        except ImportError:
            raise ImportError("pyreadr is required. Install with: pip install pyreadr")
        except Exception as e:
            raise ValueError(f"Failed to read RDS files: {e}")

        # Extract matrix dimensions
        cell_names = gt.index.astype(str)
        region_names = gt.columns.astype(str)
        gt_values = gt.values
        nrow, ncol = gt_values.shape

        # Generate allele-specific center table
        max_idx = int(np.nanmax(gt_values))
        mu0 = []
        ii = 0
        while len(mu0) < max_idx:
            ii += 1
            rho_vals = [0.5 * ii] * (ii + 1)
            theta_vals = [0] + [j / ii for j in range(1, ii + 1)]
            mu_tmp = np.column_stack([rho_vals, theta_vals])
            mu0.append(mu_tmp)

        mu0 = np.vstack(mu0)
        mu0 = pd.DataFrame(mu0[:max_idx, :], columns=["rho", "theta"])
        mu0.index = [f"center{i+1}" for i in range(mu0.shape[0])]

        mu0["Total"] = mu0["rho"] * 2
        mu0["Major"] = np.round(mu0["theta"] * mu0["Total"]).astype(int)
        mu0["Minor"] = (mu0["Total"] - mu0["Major"]).astype(int)

        # Map from center index to "Major|Minor"
        map_status = {
            idx: f"{int(row.Major)}|{int(row.Minor)}"
            for idx, row in mu0.iterrows()
        }

        # Convert genotype table to haplotype CN table
        keys = np.vectorize(
            lambda x: f"center{int(x)}" if np.isfinite(x) else np.nan
        )(gt_values)

        out = np.empty_like(keys, dtype=object)
        for i in range(nrow):
            for j in range(ncol):
                key = keys[i, j]
                out[i, j] = map_status.get(key, np.nan)

        out_t = pd.DataFrame(out.T, index=region_names, columns=cell_names)

        # Standardize segmentation table
        seg_table.columns = [c.lower() for c in seg_table.columns]
        seg_table["region_label"] = seg_table.apply(
            lambda r: f"{'chr' if self.add_chr_prefix else ''}{r['chr']}:"
                      f"{int(float(r['start'])) + (1 if self.start_plus_one else 0)}-"
                      f"{int(float(r['end']))}",
            axis=1
        )

        # Map region IDs
        map_dict = dict(
            zip(seg_table["chrr"].astype(str), seg_table["region_label"].astype(str))
        )
        out_t.index = out_t.index.map(lambda x: map_dict.get(str(x), str(x)))

        # Optional: split regions by bin size
        if self.bin_size is not None:
            print(f"[hcbench] Splitting regions by bin size = {self.bin_size}")
            new_rows, new_index = [], []
            for idx, label in enumerate(out_t.index):
                for sub in split_region(label, self.bin_size):
                    new_rows.append(out_t.iloc[idx].copy())
                    new_index.append(sub)

            out_t = pd.DataFrame(new_rows, columns=out_t.columns, index=new_index)

        out_t.index.name = "region"
        return out_t

    def run(self):
        """
        Main entry point for parsing Alleloscope output.
        Converts the RDS files into a unified haplotype CNA matrix.
        """
        print("[hcbench] Parsing Alleloscope RDS files...")
        df = self.before_pivot()

        self._check_output_path()
        output_file = os.path.join(self.output_path, "haplotype_combined.csv")
        df.to_csv(output_file)
        print(f"[hcbench] {self.__class__.__name__} parsed CNA file saved to {output_file}")

        if self.split_haplotype:
            self._postprocess_haplotype(df)

    def get_cluster(self, cluster_file_path):
        """
        Parse Alleloscope cluster assignments.

        Args:
            cluster_file_path (str): Path to the cluster assignment CSV file.

        Output:
            A CSV file 'clusters.csv' with cell-to-clone mapping.
        """
        print("[hcbench] Parsing Alleloscope cluster file...")

        cluster_df = pd.read_csv(cluster_file_path, index_col=0)
        result_df = pd.DataFrame({
            "cell_id": cluster_df.index.to_list(),
            "clone_id": cluster_df["x"]
        })

        result_file = os.path.join(self.output_path, "clusters.csv")
        result_df.to_csv(result_file, index=False)
        print(f"[hcbench] Cluster file saved to {result_file}")
